{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Importing-the-Data\" data-toc-modified-id=\"Importing-the-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Importing the Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Metadata-File\" data-toc-modified-id=\"Metadata-File-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span><code>Metadata</code> File</a></span></li><li><span><a href=\"#train_labels-File\" data-toc-modified-id=\"train_labels-File-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span><code>train_labels</code> File</a></span></li><li><span><a href=\"#Prepping-the-data-for-the-Satellite-imagery-analysis.\" data-toc-modified-id=\"Prepping-the-data-for-the-Satellite-imagery-analysis.-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Prepping the data for the Satellite imagery analysis.</a></span></li><li><span><a href=\"#Setting-up-the-DataFrame\" data-toc-modified-id=\"Setting-up-the-DataFrame-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Setting up the DataFrame</a></span></li></ul></li><li><span><a href=\"#Pulling-in-All-of-the-Data\" data-toc-modified-id=\"Pulling-in-All-of-the-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Pulling in All of the Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pulling-in-the-first-half-of-the-data.\" data-toc-modified-id=\"Pulling-in-the-first-half-of-the-data.-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Pulling in the first half of the data.</a></span></li><li><span><a href=\"#Pulling-in-the-second-half-of-the-data\" data-toc-modified-id=\"Pulling-in-the-second-half-of-the-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Pulling in the second half of the data</a></span></li><li><span><a href=\"#Pulling-in-the-third-set-of-data\" data-toc-modified-id=\"Pulling-in-the-third-set-of-data-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Pulling in the third set of data</a></span></li><li><span><a href=\"#Creating-a-Full-DataFrame\" data-toc-modified-id=\"Creating-a-Full-DataFrame-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Creating a Full DataFrame</a></span></li></ul></li><li><span><a href=\"#Test-Data\" data-toc-modified-id=\"Test-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Test Data</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running main notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.336084Z",
     "start_time": "2023-02-04T23:44:43.825368Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import geopy.distance as distance\n",
    "\n",
    "import planetary_computer as pc\n",
    "from pystac_client import Client\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# from keras.utils import load_img, img_to_array\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import rioxarray\n",
    "import cv2\n",
    "import odc.stac\n",
    "import tempfile\n",
    "import rasterio\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import functions\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.351786Z",
     "start_time": "2023-02-04T23:44:45.337086Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Can use this if I decide to use multiple satelitte images\n",
    "# def get_sat_info(df):\n",
    "\n",
    "#     '''\n",
    "#     input a dataframe and get a dictionary with satellite information for each row in the dataframe\n",
    "#     '''\n",
    "    \n",
    "#     sat_dict = {}\n",
    "#     for index in range(len(df)):\n",
    "#         row = df.iloc[index]\n",
    "\n",
    "#         # Get all satellite images\n",
    "#         search = catalog.search(collections=[\"sentinel-2-l2a\", \"landsat-c2-l2\"],\n",
    "#                                 bbox=row['bbox'],\n",
    "#                                 datetime=row['date_range'],\n",
    "#                                 query={'eo:cloud_cover': {'lt':100}}\n",
    "#     )\n",
    "\n",
    "\n",
    "#         # Going through Satellite info\n",
    "\n",
    "#         # search for sat images and create a dataframe with results for one sample\n",
    "# #         search_items = [item for item in search.get_all_items()]\n",
    "#         search_items = [item for item in search.item_collection()]\n",
    "\n",
    "\n",
    "#         pic_details = []\n",
    "#         for pic in search_items:\n",
    "#             pic_details.append(\n",
    "#             {\n",
    "#             'item': pic,\n",
    "#             'satelite_name':pic.collection_id,\n",
    "#             'img_date':pic.datetime.date(),\n",
    "#             'cloud_cover(%)': pic.properties['eo:cloud_cover'],\n",
    "#             'img_bbox': pic.bbox,\n",
    "#             'min_long': pic.bbox[0],\n",
    "#             \"max_long\": pic.bbox[2],\n",
    "#             \"min_lat\": pic.bbox[1],\n",
    "#             \"max_lat\": pic.bbox[3]\n",
    "#             }\n",
    "#             )\n",
    "\n",
    "#         temp_df = pd.DataFrame(pic_details)\n",
    "\n",
    "#         # Check to make sure sample location is actually within sat image\n",
    "#         temp_df['has_sample_point'] = (\n",
    "#             (temp_df.min_lat < row.latitude)\n",
    "#             & (temp_df.max_lat > row.latitude)\n",
    "#             & (temp_df.min_long < row.longitude)\n",
    "#             & (temp_df.max_long > row.longitude)\n",
    "#         )\n",
    "\n",
    "#         temp_df = temp_df[temp_df['has_sample_point'] == True]\n",
    "#         sat_dict[row['uid']] = temp_df\n",
    "        \n",
    "#     return sat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.367796Z",
     "start_time": "2023-02-04T23:44:45.353786Z"
    }
   },
   "outputs": [],
   "source": [
    "# # delete comments for prints (# is all on left edge)\n",
    "# def pick_best_sat(df, sat_dict):\n",
    "    \n",
    "#     '''\n",
    "#     input a dataframe and dictionary of satellite images and returns a dataframe with the best satellite image\n",
    "#     '''\n",
    "    \n",
    "#     # picking the best\n",
    "#     # inputs would need to be df and dictionary\n",
    "#     best_sat_df = pd.DataFrame()\n",
    "#     row_count=0\n",
    "#     invalid_sats = 0\n",
    "#     for index in range(len(df)):\n",
    "#         row = df.iloc[index]\n",
    "\n",
    "#         name = row['uid']\n",
    "#         temp_df = sat_dict[name]\n",
    "#         temp_df = temp_df.reset_index()\n",
    "#         # checking to see if there's only one image and adding it to df if so\n",
    "#         if len(temp_df) == 1:\n",
    "# #             print('only one satellite')\n",
    "#             temp_df = temp_df.reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#             row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#             row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#             best_sat_df = pd.concat([best_sat_df, row])\n",
    "#             row_count+=1\n",
    "\n",
    "#         # checking if no images\n",
    "#         elif len(temp_df) == 0:\n",
    "#             invalid_sats +=1\n",
    "#             row = pd.DataFrame(row).T.reset_index()\n",
    "#             row = row.set_index(pd.Series(row_count)).drop('index', axis=1)\n",
    "#             best_sat_df = pd.concat([best_sat_df, row])\n",
    "#             row_count+=1\n",
    "# #             print('no satellite images')\n",
    "#             continue\n",
    "\n",
    "#         # There are many satellite images, need to narrow it down\n",
    "#         else:\n",
    "# #             print('many sats')\n",
    "#             # first checking for any sentinel satelites\n",
    "#             if len(temp_df[temp_df['satelite_name'].str.contains('entinel')]) >0:\n",
    "#                     temp_df = temp_df[temp_df['satelite_name'].str.contains('entinel')]\n",
    "\n",
    "#                     # if only one sentinel, add to df and move on\n",
    "#                     if len(temp_df) == 1:\n",
    "# #                         print('\\tonly one sentinal')\n",
    "#                         temp_df = temp_df.reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#                         row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#                         row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#                         best_sat_df = pd.concat([best_sat_df, row])\n",
    "#                         row_count+=1\n",
    "#                     # if many sentinel, check for images with low cloud cover\n",
    "#                     else:\n",
    "# #                         print('\\tmany sentinel')\n",
    "#                         # checking for clouds less than 30%\n",
    "#                         if len(temp_df[temp_df['cloud_cover(%)'] <= 30]) >0:\n",
    "# #                             print('\\t\\tsentinal cloud cover lower than 30%')\n",
    "#                             temp_df = temp_df[temp_df['cloud_cover(%)'] <= 30]\n",
    "\n",
    "#                             # add the row with the closest date\n",
    "#                             temp_df = temp_df.sort_values('img_date', ascending=False).reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#                             temp_df = pd.DataFrame(temp_df.loc[0]).T\n",
    "#                             row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#                             row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#                             best_sat_df = pd.concat([best_sat_df, row])\n",
    "#                             row_count+=1\n",
    "#                         else:\n",
    "#                             # If there's only images with a clouds over 30%, \n",
    "#                             # pick the one with the least clouds\n",
    "# #                             print('\\t\\tvery cloudy sentinel')\n",
    "#                             temp_df = temp_df.sort_values('cloud_cover(%)', ascending=True).reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#                             temp_df = pd.DataFrame(temp_df.loc[0]).T\n",
    "#                             row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#                             row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#                             best_sat_df = pd.concat([best_sat_df, row])\n",
    "#                             row_count+=1\n",
    "\n",
    "#             else:\n",
    "# #                 print('\\tno sentinal')\n",
    "#                 if len(temp_df[temp_df['cloud_cover(%)'] <= 30]) >0:\n",
    "# #                     print('\\t\\tlandsat cloud cover lower than 30%')\n",
    "#                     temp_df = temp_df[temp_df['cloud_cover(%)'] <= 30]\n",
    "\n",
    "#                     # add the row with the closest date\n",
    "#                     temp_df = temp_df.sort_values('img_date', ascending=False).reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#                     temp_df = pd.DataFrame(temp_df.loc[0]).T\n",
    "#                     row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#                     row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#                     best_sat_df = pd.concat([best_sat_df, row])\n",
    "#                     row_count+=1\n",
    "#                 else:\n",
    "#                     # If there's only images with a clouds over 30%, \n",
    "#                     # pick the one with the least clouds\n",
    "# #                     print('\\t\\tvery cloudy landsat')\n",
    "#                     temp_df = temp_df.sort_values('cloud_cover(%)', ascending=True).reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#                     temp_df = pd.DataFrame(temp_df.loc[0]).T\n",
    "#                     row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#                     row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#                     best_sat_df = pd.concat([best_sat_df, row])\n",
    "#                     row_count+=1\n",
    "\n",
    "\n",
    "\n",
    "#     print(f'{len(df)} attempts. {invalid_sats} failures.')\n",
    "#     return best_sat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.383875Z",
     "start_time": "2023-02-04T23:44:45.368800Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_arrays_from_sats(df):\n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     input a dataframe with satellites in it and get a dictionary with arrays \n",
    "#     that came from cropped images around the sample area\n",
    "#     '''\n",
    "\n",
    "# # Now to get images from the satellites\n",
    "#     array_dict = {}\n",
    "#     scaler = functions.MinMaxScaler3D(feature_range=(0,255))\n",
    "#     error_count = 0\n",
    "#     attempt_count = 0\n",
    "#     for index in range(len(df)):\n",
    "#         row = df.iloc[index]\n",
    "\n",
    "\n",
    "#         try:\n",
    "#             attempt_count +=1\n",
    "#         # checking to see which satellite it came from\n",
    "#             if 'sentinel' in row['satelite_name']:\n",
    "#                 # Setting tiny crop box for image\n",
    "#                 minx, miny, maxx, maxy = row['tiny_crop_bbox']\n",
    "#                 # getting the image\n",
    "#                 image = rioxarray.open_rasterio(pc.sign(row['item'].assets[\"visual\"].href)).rio.clip_box(\n",
    "#                         minx=minx,\n",
    "#                         miny=miny,\n",
    "#                         maxx=maxx,\n",
    "#                         maxy=maxy,\n",
    "#                         crs=\"EPSG:4326\",\n",
    "#                     )\n",
    "\n",
    "#                 image_array = image.to_numpy()\n",
    "#                 img_array_trans = np.transpose(image_array, axes=[1, 2, 0])\n",
    "#                 # storing array of image in dictionary\n",
    "#                 array_dict[row['uid']] = img_array_trans\n",
    "\n",
    "#             else:\n",
    "#                 # getting the image from the LandSat satellite\n",
    "#                 minx, miny, maxx, maxy = row['tiny_crop_bbox']\n",
    "#                 image = odc.stac.stac_load(\n",
    "#                         [pc.sign(row['item'])], bands=[\"red\", \"green\", \"blue\"], bbox=[minx, miny, maxx, maxy]\n",
    "#                     ).isel(time=0)\n",
    "\n",
    "#                 image_array = image[[\"red\", \"green\", \"blue\"]].to_array()\n",
    "#                 img_array_trans = np.transpose(image_array.to_numpy(), axes=[1, 2, 0])\n",
    "#                 # scaling the image so its the same scale as the sentinel ones\n",
    "#                 scaled_img = scaler.fit_transform(img_array_trans)\n",
    "#         #         int_scaled_img = scaled_img.astype(int)\n",
    "#                 # storing array of image in dictionary\n",
    "#                 array_dict[row['uid']] = scaled_img\n",
    "                \n",
    "\n",
    "#         except:\n",
    "#             error_count +=1\n",
    "            \n",
    "            \n",
    "#     print(f'{attempt_count} attempted. {error_count} failures.')\n",
    "#     return array_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.399875Z",
     "start_time": "2023-02-04T23:44:45.384876Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_features(df, img_arrays):\n",
    "#     '''\n",
    "#     input a dataframe and a list of integers and create features from arrays\n",
    "#     '''\n",
    "#     feature_df = pd.DataFrame()\n",
    "#     for index in range(len(img_arrays.keys())):\n",
    "#         feature_dict = {}\n",
    "#         key =list(img_arrays.keys())[index]\n",
    "# #         row = df.iloc[index]\n",
    "#         temp_array = img_arrays[key]\n",
    "#         for n, color in enumerate(['red', 'green', 'blue']):\n",
    "#             feature_dict['uid'] = key\n",
    "#             feature_dict[f'{color}_mean'] = np.mean(temp_array[:,:,n])\n",
    "#             feature_dict[f'{color}_median'] = np.median(temp_array[:,:,n])\n",
    "#             feature_dict[f'{color}_max'] = np.max(temp_array[:,:,n])\n",
    "#             feature_dict[f'{color}_min'] = np.min(temp_array[:,:,n])\n",
    "#             feature_dict[f'{color}_sum'] = np.sum(temp_array[:,:,n])\n",
    "#             feature_dict[f'{color}_product'] = np.prod(temp_array[:,:,n])\n",
    "#         feature_df = pd.concat([feature_df, pd.DataFrame(feature_dict, index=[index])], )\n",
    "\n",
    "#     feature_df = df.merge(feature_df, how='outer', on='uid')\n",
    "#     return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.415876Z",
     "start_time": "2023-02-04T23:44:45.400876Z"
    }
   },
   "outputs": [],
   "source": [
    "# # A function to get it all in one\n",
    "# def get_sat_to_features(df):\n",
    "    \n",
    "#     '''\n",
    "#     input a dataframe of raw data and get sat images, convert to arrays, and turn into features.\n",
    "#     '''\n",
    "#     catalog = Client.open(\n",
    "#     \"https://planetarycomputer.microsoft.com/api/stac/v1\", modifier=pc.sign_inplace\n",
    "#     )\n",
    "    \n",
    "#     # get sat info\n",
    "#     satelite_dict = get_sat_info(df)\n",
    "    \n",
    "#     # pick best sat\n",
    "#     single_df = pick_best_sat(df, satelite_dict)\n",
    "    \n",
    "#     # get image arrays from best sats\n",
    "#     img_arrays = get_arrays_from_sats(single_df)\n",
    "    \n",
    "#     # get a dataframe with relevant features\n",
    "#     feature_df = get_features(single_df, img_arrays)\n",
    "    \n",
    "#     return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.431875Z",
     "start_time": "2023-02-04T23:44:45.416876Z"
    }
   },
   "outputs": [],
   "source": [
    "# def clean_data(df):\n",
    "#     '''\n",
    "#     input dataframe with all data and clean it.\n",
    "#     '''\n",
    "#     # only keeping cols that I need\n",
    "#     model_df = df[['date', 'latitude', 'longitude', 'season', 'img_date',\n",
    "#             'red_mean', 'red_median', 'red_max', 'red_min','red_sum',\n",
    "#             'red_product', 'green_mean', 'green_median', 'green_max',\n",
    "#             'green_min', 'green_sum', 'green_product', 'blue_mean',\n",
    "#             'blue_median','blue_max', 'blue_min', 'blue_sum', 'blue_product', 'severity']]\n",
    "#     # dropping nulls\n",
    "#     model_df = model_df.dropna()\n",
    "#     # converting to correct type\n",
    "#     model_df['date'] = model_df['date'].apply(lambda x: datetime.date(x))\n",
    "#     # getting difference from image date to sample date and creating feature\n",
    "#     model_df['days_from_sat_to_sample'] = model_df['date'] - model_df['img_date']\n",
    "#     # converting to int\n",
    "#     model_df['days_from_sat_to_sample'] = model_df['days_from_sat_to_sample'].dt.days\n",
    "#     # converting from datetime to an int\n",
    "#     model_df['date'] = model_df['date'].apply(lambda x: x.toordinal())\n",
    "#     model_df['img_date'] = model_df['img_date'].apply(lambda x: x.toordinal())\n",
    "#     # converting from string to float\n",
    "#     model_df['latitude'] = model_df['latitude'].apply(lambda x: x.astype(float))\n",
    "#     model_df['longitude'] = model_df['longitude'].apply(lambda x: x.astype(float))\n",
    "    \n",
    "#     # One hot encoding seasons\n",
    "#     ohe = OneHotEncoder(sparse=False)\n",
    "#     seasons = ohe.fit_transform(model_df[['season']])\n",
    "#     cols = ohe.get_feature_names_out()\n",
    "#     # converting new ohe to dataframe\n",
    "#     seasons = pd.DataFrame(seasons, columns=cols, index=model_df.index)\n",
    "#     model_ohe = pd.concat([model_df.drop('season', axis=1), seasons], axis=1)\n",
    "#     return model_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Metadata` File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.463879Z",
     "start_time": "2023-02-04T23:44:45.431875Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading in the data and bringing in date as datetime dtype\n",
    "metadata = pd.read_csv('Data/metadata.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train_labels` File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.479932Z",
     "start_time": "2023-02-04T23:44:45.465884Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('Data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping the data for the Satellite imagery analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.495948Z",
     "start_time": "2023-02-04T23:44:45.480934Z"
    }
   },
   "outputs": [],
   "source": [
    "sat_df = metadata.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.510949Z",
     "start_time": "2023-02-04T23:44:45.496952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    17060\n",
       "test      6510\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sat_df['split'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.526970Z",
     "start_time": "2023-02-04T23:44:45.511951Z"
    }
   },
   "outputs": [],
   "source": [
    "sat_train = sat_df[sat_df['split'] == 'train'].copy()\n",
    "sat_test = sat_df[sat_df['split'] == 'test'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing back in the labels for sat_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.542983Z",
     "start_time": "2023-02-04T23:44:45.527973Z"
    }
   },
   "outputs": [],
   "source": [
    "sat_train = sat_train.merge(train_labels, on='uid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I use a custom function to add a date range that the satellites can interpret and also include bounding boxes to later manipulate the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.559000Z",
     "start_time": "2023-02-04T23:44:45.543983Z"
    }
   },
   "outputs": [],
   "source": [
    "# functions.get_important_info(sat_train, dist=31, big_crop_dist=3000, small_crop_dist=500, tiny_crop_dist=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling in All of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the API is sometimes unstable. I will be pulling over the data in two large batches with several smaller batches making up the larger batches. I am splitting the data into batches below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.574506Z",
     "start_time": "2023-02-04T23:44:45.560002Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_train = list(np.arange(0, len(sat_train), 853))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.590526Z",
     "start_time": "2023-02-04T23:44:45.575511Z"
    }
   },
   "outputs": [],
   "source": [
    "# first_half = all_train[:int(len(all_train)/2)]\n",
    "\n",
    "# first_half_dict = {}\n",
    "# for batch in range(1, len(first_half)):\n",
    "#     first_half_dict[f'sat_train_{batch}'] = sat_train[first_half[batch-1]:first_half[batch]]\n",
    "    \n",
    "# first_half_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.605788Z",
     "start_time": "2023-02-04T23:44:45.591528Z"
    }
   },
   "outputs": [],
   "source": [
    "# first_half_dict['sat_train_9'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.621299Z",
     "start_time": "2023-02-04T23:44:45.606791Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_train.append(len(sat_train))\n",
    "# second_half = all_train[9:]\n",
    "\n",
    "# second_half_dict = {}\n",
    "# for batch in range(10, len(second_half)+9):\n",
    "#     second_half_dict[f'sat_train_{batch}'] = sat_train[second_half[batch-10]:second_half[batch-9]]\n",
    "    \n",
    "# second_half_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.636299Z",
     "start_time": "2023-02-04T23:44:45.622301Z"
    }
   },
   "outputs": [],
   "source": [
    "# second_half_dict['sat_train_20'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling in the first half of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.652322Z",
     "start_time": "2023-02-04T23:44:45.637301Z"
    }
   },
   "outputs": [],
   "source": [
    "# commented out due to having completed and pickled the results\n",
    "# first_half_key_list = list(first_half_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.667790Z",
     "start_time": "2023-02-04T23:44:45.653328Z"
    }
   },
   "outputs": [],
   "source": [
    "# commented out due to having completed and pickled the results\n",
    "\n",
    "\n",
    "# first_half_results_dict = {}\n",
    "# for n, key in enumerate(first_half_key_list):\n",
    "#     first_half_results_dict[key] = get_sat_to_features(first_half_dict[key])\n",
    "#     print(f\"{key} has finished loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling in the second half of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.683801Z",
     "start_time": "2023-02-04T23:44:45.668791Z"
    }
   },
   "outputs": [],
   "source": [
    "# second_half_key_list = list(second_half_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.699800Z",
     "start_time": "2023-02-04T23:44:45.684802Z"
    }
   },
   "outputs": [],
   "source": [
    "# second_half_results_dict = {}\n",
    "# for key in second_half_key_list:\n",
    "#     second_half_results_dict[key] = get_sat_to_features(second_half_dict[key])\n",
    "#     print(f\"{key} has finished loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.714806Z",
     "start_time": "2023-02-04T23:44:45.700800Z"
    }
   },
   "outputs": [],
   "source": [
    "# second_half_results_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling in the third set of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T21:23:42.776569Z",
     "start_time": "2023-02-03T21:23:42.761561Z"
    }
   },
   "source": [
    "The API is incredibly finicky. Only sat_train_10-sat_train_13 was successful. I am running another pull request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.730815Z",
     "start_time": "2023-02-04T23:44:45.715806Z"
    }
   },
   "outputs": [],
   "source": [
    "# third_pull_key_list = second_half_key_list[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.745822Z",
     "start_time": "2023-02-04T23:44:45.731814Z"
    }
   },
   "outputs": [],
   "source": [
    "# third_pull_results_dict = {}\n",
    "# for key in third_pull_key_list:\n",
    "#     third_pull_results_dict[key] = get_sat_to_features(second_half_dict[key])\n",
    "#     print(f\"{key} has finished loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Full DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the data in a dictionary, I will concat it all together into a complete dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.761847Z",
     "start_time": "2023-02-04T23:44:45.748823Z"
    }
   },
   "outputs": [],
   "source": [
    "# commented out due to having completed and pickled the results\n",
    "# First pull to DataFrame\n",
    "# first_pull_df = pd.concat(first_half_results_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.777348Z",
     "start_time": "2023-02-04T23:44:45.762846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Second pull to DataFrame\n",
    "# second_pull_df = pd.concat(second_half_results_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.792359Z",
     "start_time": "2023-02-04T23:44:45.778348Z"
    }
   },
   "outputs": [],
   "source": [
    "# Third pull to DataFrame\n",
    "# third_pull_df = pd.concat(third_pull_results_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also need to store the pulled data as a .pkl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.808400Z",
     "start_time": "2023-02-04T23:44:45.793359Z"
    }
   },
   "outputs": [],
   "source": [
    "# first_pull_df.to_pickle('./first_7677_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.824407Z",
     "start_time": "2023-02-04T23:44:45.809401Z"
    }
   },
   "outputs": [],
   "source": [
    "# first_pull_df = pd.read_pickle('./first_7677_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.840427Z",
     "start_time": "2023-02-04T23:44:45.824407Z"
    }
   },
   "outputs": [],
   "source": [
    "# second_pull_df.to_pickle('./second_set_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.855427Z",
     "start_time": "2023-02-04T23:44:45.841430Z"
    }
   },
   "outputs": [],
   "source": [
    "# second_pull_df = pd.read_pickle('./second_set_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.871160Z",
     "start_time": "2023-02-04T23:44:45.856433Z"
    }
   },
   "outputs": [],
   "source": [
    "# third_pull_df.to_pickle('./third_set_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.886173Z",
     "start_time": "2023-02-04T23:44:45.872162Z"
    }
   },
   "outputs": [],
   "source": [
    "# third_pull_df = pd.read_pickle('./third_set_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.901778Z",
     "start_time": "2023-02-04T23:44:45.887175Z"
    }
   },
   "outputs": [],
   "source": [
    "# full_df = pd.concat([first_pull_df, second_pull_df, third_pull_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.917781Z",
     "start_time": "2023-02-04T23:44:45.902784Z"
    }
   },
   "outputs": [],
   "source": [
    "# full_df.to_pickle('./full_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:45.933781Z",
     "start_time": "2023-02-04T23:44:45.918783Z"
    }
   },
   "outputs": [],
   "source": [
    "# new_full = pd.read_pickle('../full_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:54.211728Z",
     "start_time": "2023-02-04T23:44:45.934782Z"
    }
   },
   "outputs": [],
   "source": [
    "sat_test = functions.get_important_info(sat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:44:54.227738Z",
     "start_time": "2023-02-04T23:44:54.212729Z"
    }
   },
   "outputs": [],
   "source": [
    "small_sat_test = sat_test[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:45:03.953145Z",
     "start_time": "2023-02-04T23:44:54.228740Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index(['level_0'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sat_to_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmall_sat_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\flatiron\\important_repo\\final_project\\Detecting-Harmful-Algal-Blooms\\functions.py:470\u001b[0m, in \u001b[0;36mget_sat_to_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    467\u001b[0m satelite_dict \u001b[38;5;241m=\u001b[39m get_sat_info(df)\n\u001b[0;32m    469\u001b[0m \u001b[38;5;66;03m# pick best sat\u001b[39;00m\n\u001b[1;32m--> 470\u001b[0m single_df \u001b[38;5;241m=\u001b[39m \u001b[43mpick_best_sat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msatelite_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# get image arrays from best sats\u001b[39;00m\n\u001b[0;32m    473\u001b[0m img_arrays \u001b[38;5;241m=\u001b[39m get_arrays_from_sats(single_df)\n",
      "File \u001b[1;32m~\\Desktop\\flatiron\\important_repo\\final_project\\Detecting-Harmful-Algal-Blooms\\functions.py:315\u001b[0m, in \u001b[0;36mpick_best_sat\u001b[1;34m(df, sat_dict)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(temp_df) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    314\u001b[0m     temp_df \u001b[38;5;241m=\u001b[39m temp_df\u001b[38;5;241m.\u001b[39mreset_index()\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_long\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_long\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_lat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_lat\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 315\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mouter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m     row \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mset_index(pd\u001b[38;5;241m.\u001b[39mSeries(row_count))\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel_0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    317\u001b[0m     best_sat_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([best_sat_df, row])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\core\\frame.py:9979\u001b[0m, in \u001b[0;36mDataFrame.join\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort, validate)\u001b[0m\n\u001b[0;32m   9816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\n\u001b[0;32m   9817\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9818\u001b[0m     other: DataFrame \u001b[38;5;241m|\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[DataFrame \u001b[38;5;241m|\u001b[39m Series],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9824\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   9825\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   9826\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   9827\u001b[0m \u001b[38;5;124;03m    Join columns of another DataFrame.\u001b[39;00m\n\u001b[0;32m   9828\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9977\u001b[0m \u001b[38;5;124;03m    5  K1  A5   B1\u001b[39;00m\n\u001b[0;32m   9978\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 9979\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_join_compat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlsuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlsuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrsuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrsuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9985\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9987\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\core\\frame.py:10018\u001b[0m, in \u001b[0;36mDataFrame._join_compat\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort, validate)\u001b[0m\n\u001b[0;32m  10008\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m  10009\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[0;32m  10010\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10011\u001b[0m             other,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10016\u001b[0m             validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m  10017\u001b[0m         )\n\u001b[1;32m> 10018\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10019\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m  10024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m  10025\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlsuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrsuffix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10026\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10028\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10029\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m  10030\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:124\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    109\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m    110\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    111\u001b[0m         left,\n\u001b[0;32m    112\u001b[0m         right,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    123\u001b[0m     )\n\u001b[1;32m--> 124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:775\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    771\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[0;32m    773\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[1;32m--> 775\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    778\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:729\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[0;32m    726\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft[:]\n\u001b[0;32m    727\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright[:]\n\u001b[1;32m--> 729\u001b[0m llabels, rlabels \u001b[38;5;241m=\u001b[39m \u001b[43m_items_overlap_with_suffix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuffixes\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001b[39;00m\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;66;03m#  a MultiIndex for left.index.\u001b[39;00m\n\u001b[0;32m    737\u001b[0m     lmgr \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    738\u001b[0m         join_index,\n\u001b[0;32m    739\u001b[0m         left_indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    744\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    745\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:2458\u001b[0m, in \u001b[0;36m_items_overlap_with_suffix\u001b[1;34m(left, right, suffixes)\u001b[0m\n\u001b[0;32m   2455\u001b[0m lsuffix, rsuffix \u001b[38;5;241m=\u001b[39m suffixes\n\u001b[0;32m   2457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lsuffix \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rsuffix:\n\u001b[1;32m-> 2458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns overlap but no suffix specified: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mto_rename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrenamer\u001b[39m(x, suffix):\n\u001b[0;32m   2461\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2462\u001b[0m \u001b[38;5;124;03m    Rename the left and right indices.\u001b[39;00m\n\u001b[0;32m   2463\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2474\u001b[0m \u001b[38;5;124;03m    x : renamed column name\u001b[39;00m\n\u001b[0;32m   2475\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: columns overlap but no suffix specified: Index(['level_0'], dtype='object')"
     ]
    }
   ],
   "source": [
    "test_df = functions.get_sat_to_features(small_sat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (capstone)",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
