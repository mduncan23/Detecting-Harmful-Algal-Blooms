{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Importing-the-Data\" data-toc-modified-id=\"Importing-the-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Importing the Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Metadata-File\" data-toc-modified-id=\"Metadata-File-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span><code>Metadata</code> File</a></span></li><li><span><a href=\"#train_labels-File\" data-toc-modified-id=\"train_labels-File-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span><code>train_labels</code> File</a></span></li><li><span><a href=\"#Prepping-the-data-for-the-Satellite-imagery-analysis.\" data-toc-modified-id=\"Prepping-the-data-for-the-Satellite-imagery-analysis.-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Prepping the data for the Satellite imagery analysis.</a></span></li><li><span><a href=\"#Setting-up-the-DataFrame\" data-toc-modified-id=\"Setting-up-the-DataFrame-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Setting up the DataFrame</a></span></li></ul></li><li><span><a href=\"#Pulling-in-All-of-the-Data\" data-toc-modified-id=\"Pulling-in-All-of-the-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Pulling in All of the Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pulling-in-the-first-half-of-the-data.\" data-toc-modified-id=\"Pulling-in-the-first-half-of-the-data.-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Pulling in the first half of the data.</a></span></li><li><span><a href=\"#Pulling-in-the-second-half-of-the-data\" data-toc-modified-id=\"Pulling-in-the-second-half-of-the-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Pulling in the second half of the data</a></span></li><li><span><a href=\"#Pulling-in-the-third-set-of-data\" data-toc-modified-id=\"Pulling-in-the-third-set-of-data-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Pulling in the third set of data</a></span></li><li><span><a href=\"#Creating-a-Full-DataFrame\" data-toc-modified-id=\"Creating-a-Full-DataFrame-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Creating a Full DataFrame</a></span></li></ul></li><li><span><a href=\"#Test-Data\" data-toc-modified-id=\"Test-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Test Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pulling-the-the-Data-in-Batches-and-Saving-to-.pkl\" data-toc-modified-id=\"Pulling-the-the-Data-in-Batches-and-Saving-to-.pkl-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Pulling the the Data in Batches and Saving to .pkl</a></span></li><li><span><a href=\"#Reading-in-the-Saved-Test-Data\" data-toc-modified-id=\"Reading-in-the-Saved-Test-Data-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Reading in the Saved Test Data</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running main notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T20:29:38.069824Z",
     "start_time": "2023-02-05T20:29:36.428113Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import geopy.distance as distance\n",
    "\n",
    "import planetary_computer as pc\n",
    "from pystac_client import Client\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# from keras.utils import load_img, img_to_array\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import rioxarray\n",
    "import cv2\n",
    "import odc.stac\n",
    "import tempfile\n",
    "import rasterio\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import functions\n",
    "import functions2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Metadata` File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T20:30:38.827004Z",
     "start_time": "2023-02-05T20:30:38.806991Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading in the data and bringing in date as datetime dtype\n",
    "metadata = pd.read_csv('Data/metadata.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train_labels` File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T20:30:40.169308Z",
     "start_time": "2023-02-05T20:30:40.155283Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('Data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping the data for the Satellite imagery analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T20:30:41.327885Z",
     "start_time": "2023-02-05T20:30:41.314871Z"
    }
   },
   "outputs": [],
   "source": [
    "sat_df = metadata.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T20:30:42.263472Z",
     "start_time": "2023-02-05T20:30:42.243084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    17060\n",
       "test      6510\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sat_df['split'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T20:30:42.997002Z",
     "start_time": "2023-02-05T20:30:42.978992Z"
    }
   },
   "outputs": [],
   "source": [
    "sat_train = sat_df[sat_df['split'] == 'train'].copy()\n",
    "sat_test = sat_df[sat_df['split'] == 'test'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing back in the labels for sat_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T20:30:44.103312Z",
     "start_time": "2023-02-05T20:30:44.083741Z"
    }
   },
   "outputs": [],
   "source": [
    "sat_train = sat_train.merge(train_labels, on='uid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I use a custom function to add a date range that the satellites can interpret and also include bounding boxes to later manipulate the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T20:31:12.695012Z",
     "start_time": "2023-02-05T20:30:51.082622Z"
    }
   },
   "outputs": [],
   "source": [
    "functions.get_important_info(sat_train, dist=31, big_crop_dist=3000, small_crop_dist=500, tiny_crop_dist=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling in All of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the API is sometimes unstable. I will be pulling over the data in two large batches with several smaller batches making up the larger batches. I am splitting the data into batches below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T20:31:16.495942Z",
     "start_time": "2023-02-05T20:31:16.490945Z"
    }
   },
   "outputs": [],
   "source": [
    "all_train = list(np.arange(0, len(sat_train), 853))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T20:32:24.210098Z",
     "start_time": "2023-02-05T20:32:24.202590Z"
    }
   },
   "outputs": [],
   "source": [
    "all_train.append(17060)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T20:32:28.463250Z",
     "start_time": "2023-02-05T20:32:28.459249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 853,\n",
       " 1706,\n",
       " 2559,\n",
       " 3412,\n",
       " 4265,\n",
       " 5118,\n",
       " 5971,\n",
       " 6824,\n",
       " 7677,\n",
       " 8530,\n",
       " 9383,\n",
       " 10236,\n",
       " 11089,\n",
       " 11942,\n",
       " 12795,\n",
       " 13648,\n",
       " 14501,\n",
       " 15354,\n",
       " 16207,\n",
       " 17060]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T20:37:40.277892Z",
     "start_time": "2023-02-05T20:37:40.266874Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dict = {}\n",
    "for slice in range(1, len(all_train)):\n",
    "    train_dict[f\"sat_train_{slice}\"] = sat_train[all_train[slice-1]:all_train[slice]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T20:38:50.861621Z",
     "start_time": "2023-02-05T20:38:50.858621Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dict_key_list = list(train_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T20:38:19.941011Z",
     "start_time": "2023-02-05T20:38:19.922315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>split</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "      <th>density</th>\n",
       "      <th>date_range</th>\n",
       "      <th>bbox</th>\n",
       "      <th>big_crop_bbox</th>\n",
       "      <th>small_crop_bbox</th>\n",
       "      <th>tiny_crop_bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>aabm</td>\n",
       "      <td>39.080319</td>\n",
       "      <td>-86.430867</td>\n",
       "      <td>2018-05-14</td>\n",
       "      <td>train</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1</td>\n",
       "      <td>585.0</td>\n",
       "      <td>2018-04-29/2018-05-14</td>\n",
       "      <td>[-87.00742888244132, 38.63091417147125, -85.85...</td>\n",
       "      <td>[-86.46553737052635, 39.05329612116674, -86.39...</td>\n",
       "      <td>[-86.43664511758249, 39.07581525298953, -86.42...</td>\n",
       "      <td>[-86.43202235685135, 39.079418305988106, -86.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>aacd</td>\n",
       "      <td>35.875083</td>\n",
       "      <td>-78.878434</td>\n",
       "      <td>2020-11-19</td>\n",
       "      <td>train</td>\n",
       "      <td>south</td>\n",
       "      <td>1</td>\n",
       "      <td>290.0</td>\n",
       "      <td>2020-11-04/2020-11-19</td>\n",
       "      <td>[-79.43088170919651, 35.425434522510464, -78.3...</td>\n",
       "      <td>[-78.91165478658658, 35.84804560208817, -78.84...</td>\n",
       "      <td>[-78.88397103218583, 35.8705769758293, -78.872...</td>\n",
       "      <td>[-78.87954163128398, 35.87418198776951, -78.87...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>aaee</td>\n",
       "      <td>35.487000</td>\n",
       "      <td>-79.062133</td>\n",
       "      <td>2016-08-24</td>\n",
       "      <td>train</td>\n",
       "      <td>south</td>\n",
       "      <td>1</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>2016-08-09/2016-08-24</td>\n",
       "      <td>[-79.61191193921022, 35.03732231399556, -78.51...</td>\n",
       "      <td>[-79.09519299105324, 35.459960615407574, -79.0...</td>\n",
       "      <td>[-79.06764296370947, 35.48249344433146, -79.05...</td>\n",
       "      <td>[-79.06323495914324, 35.48609868913609, -79.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>aaff</td>\n",
       "      <td>38.049471</td>\n",
       "      <td>-99.827001</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>train</td>\n",
       "      <td>midwest</td>\n",
       "      <td>3</td>\n",
       "      <td>111825.0</td>\n",
       "      <td>2019-07-08/2019-07-23</td>\n",
       "      <td>[-100.3953854756163, 37.59998670596361, -99.25...</td>\n",
       "      <td>[-99.86118001864864, 38.02244310076497, -99.79...</td>\n",
       "      <td>[-99.83269759502433, 38.044966208779, -99.8213...</td>\n",
       "      <td>[-99.82814040700623, 38.048569898032675, -99.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>aafl</td>\n",
       "      <td>39.474744</td>\n",
       "      <td>-86.898353</td>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>train</td>\n",
       "      <td>midwest</td>\n",
       "      <td>4</td>\n",
       "      <td>2017313.0</td>\n",
       "      <td>2021-08-08/2021-08-23</td>\n",
       "      <td>[-87.47815708269697, 39.02536958186914, -86.31...</td>\n",
       "      <td>[-86.93321866191961, 39.44772288780534, -86.86...</td>\n",
       "      <td>[-86.9041639439351, 39.47024049004548, -86.892...</td>\n",
       "      <td>[-86.89951518878857, 39.473843298288934, -86.8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index   uid   latitude  longitude       date  split   region  severity  \\\n",
       "0      0  aabm  39.080319 -86.430867 2018-05-14  train  midwest         1   \n",
       "1      2  aacd  35.875083 -78.878434 2020-11-19  train    south         1   \n",
       "2      3  aaee  35.487000 -79.062133 2016-08-24  train    south         1   \n",
       "3      4  aaff  38.049471 -99.827001 2019-07-23  train  midwest         3   \n",
       "4      5  aafl  39.474744 -86.898353 2021-08-23  train  midwest         4   \n",
       "\n",
       "     density             date_range  \\\n",
       "0      585.0  2018-04-29/2018-05-14   \n",
       "1      290.0  2020-11-04/2020-11-19   \n",
       "2     1614.0  2016-08-09/2016-08-24   \n",
       "3   111825.0  2019-07-08/2019-07-23   \n",
       "4  2017313.0  2021-08-08/2021-08-23   \n",
       "\n",
       "                                                bbox  \\\n",
       "0  [-87.00742888244132, 38.63091417147125, -85.85...   \n",
       "1  [-79.43088170919651, 35.425434522510464, -78.3...   \n",
       "2  [-79.61191193921022, 35.03732231399556, -78.51...   \n",
       "3  [-100.3953854756163, 37.59998670596361, -99.25...   \n",
       "4  [-87.47815708269697, 39.02536958186914, -86.31...   \n",
       "\n",
       "                                       big_crop_bbox  \\\n",
       "0  [-86.46553737052635, 39.05329612116674, -86.39...   \n",
       "1  [-78.91165478658658, 35.84804560208817, -78.84...   \n",
       "2  [-79.09519299105324, 35.459960615407574, -79.0...   \n",
       "3  [-99.86118001864864, 38.02244310076497, -99.79...   \n",
       "4  [-86.93321866191961, 39.44772288780534, -86.86...   \n",
       "\n",
       "                                     small_crop_bbox  \\\n",
       "0  [-86.43664511758249, 39.07581525298953, -86.42...   \n",
       "1  [-78.88397103218583, 35.8705769758293, -78.872...   \n",
       "2  [-79.06764296370947, 35.48249344433146, -79.05...   \n",
       "3  [-99.83269759502433, 38.044966208779, -99.8213...   \n",
       "4  [-86.9041639439351, 39.47024049004548, -86.892...   \n",
       "\n",
       "                                      tiny_crop_bbox  \n",
       "0  [-86.43202235685135, 39.079418305988106, -86.4...  \n",
       "1  [-78.87954163128398, 35.87418198776951, -78.87...  \n",
       "2  [-79.06323495914324, 35.48609868913609, -79.06...  \n",
       "3  [-99.82814040700623, 38.048569898032675, -99.8...  \n",
       "4  [-86.89951518878857, 39.473843298288934, -86.8...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict['sat_train_1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.426959Z",
     "start_time": "2023-02-04T23:48:29.411955Z"
    }
   },
   "outputs": [],
   "source": [
    "# first_half = all_train[:int(len(all_train)/2)]\n",
    "\n",
    "# first_half_dict = {}\n",
    "# for batch in range(1, len(first_half)):\n",
    "#     first_half_dict[f'sat_train_{batch}'] = sat_train[first_half[batch-1]:first_half[batch]]\n",
    "    \n",
    "# first_half_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.442961Z",
     "start_time": "2023-02-04T23:48:29.427960Z"
    }
   },
   "outputs": [],
   "source": [
    "# first_half_dict['sat_train_9'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.458969Z",
     "start_time": "2023-02-04T23:48:29.442961Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_train.append(len(sat_train))\n",
    "# second_half = all_train[9:]\n",
    "\n",
    "# second_half_dict = {}\n",
    "# for batch in range(10, len(second_half)+9):\n",
    "#     second_half_dict[f'sat_train_{batch}'] = sat_train[second_half[batch-10]:second_half[batch-9]]\n",
    "    \n",
    "# second_half_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.474983Z",
     "start_time": "2023-02-04T23:48:29.459971Z"
    }
   },
   "outputs": [],
   "source": [
    "# second_half_dict['sat_train_20'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling in the first half of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.489930Z",
     "start_time": "2023-02-04T23:48:29.474983Z"
    }
   },
   "outputs": [],
   "source": [
    "# commented out due to having completed and pickled the results\n",
    "# first_half_key_list = list(first_half_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.505938Z",
     "start_time": "2023-02-04T23:48:29.490933Z"
    }
   },
   "outputs": [],
   "source": [
    "# commented out due to having completed and pickled the results\n",
    "\n",
    "\n",
    "# first_half_results_dict = {}\n",
    "# for n, key in enumerate(first_half_key_list):\n",
    "#     first_half_results_dict[key] = get_sat_to_features(first_half_dict[key])\n",
    "#     print(f\"{key} has finished loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling in the second half of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.521447Z",
     "start_time": "2023-02-04T23:48:29.506939Z"
    }
   },
   "outputs": [],
   "source": [
    "# second_half_key_list = list(second_half_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.536447Z",
     "start_time": "2023-02-04T23:48:29.522448Z"
    }
   },
   "outputs": [],
   "source": [
    "# second_half_results_dict = {}\n",
    "# for key in second_half_key_list:\n",
    "#     second_half_results_dict[key] = get_sat_to_features(second_half_dict[key])\n",
    "#     print(f\"{key} has finished loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.552447Z",
     "start_time": "2023-02-04T23:48:29.537448Z"
    }
   },
   "outputs": [],
   "source": [
    "# second_half_results_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling in the third set of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T21:23:42.776569Z",
     "start_time": "2023-02-03T21:23:42.761561Z"
    }
   },
   "source": [
    "The API is incredibly finicky. Only sat_train_10-sat_train_13 was successful. I am running another pull request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.568447Z",
     "start_time": "2023-02-04T23:48:29.553448Z"
    }
   },
   "outputs": [],
   "source": [
    "# third_pull_key_list = second_half_key_list[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.584447Z",
     "start_time": "2023-02-04T23:48:29.569447Z"
    }
   },
   "outputs": [],
   "source": [
    "# third_pull_results_dict = {}\n",
    "# for key in third_pull_key_list:\n",
    "#     third_pull_results_dict[key] = get_sat_to_features(second_half_dict[key])\n",
    "#     print(f\"{key} has finished loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Full DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the data in a dictionary, I will concat it all together into a complete dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.600451Z",
     "start_time": "2023-02-04T23:48:29.585448Z"
    }
   },
   "outputs": [],
   "source": [
    "# commented out due to having completed and pickled the results\n",
    "# First pull to DataFrame\n",
    "# first_pull_df = pd.concat(first_half_results_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.615956Z",
     "start_time": "2023-02-04T23:48:29.603451Z"
    }
   },
   "outputs": [],
   "source": [
    "# Second pull to DataFrame\n",
    "# second_pull_df = pd.concat(second_half_results_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.631956Z",
     "start_time": "2023-02-04T23:48:29.616957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Third pull to DataFrame\n",
    "# third_pull_df = pd.concat(third_pull_results_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also need to store the pulled data as a .pkl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.647955Z",
     "start_time": "2023-02-04T23:48:29.632956Z"
    }
   },
   "outputs": [],
   "source": [
    "# first_pull_df.to_pickle('./first_7677_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.663222Z",
     "start_time": "2023-02-04T23:48:29.648955Z"
    }
   },
   "outputs": [],
   "source": [
    "# first_pull_df = pd.read_pickle('./first_7677_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.678234Z",
     "start_time": "2023-02-04T23:48:29.664225Z"
    }
   },
   "outputs": [],
   "source": [
    "# second_pull_df.to_pickle('./second_set_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.694258Z",
     "start_time": "2023-02-04T23:48:29.679232Z"
    }
   },
   "outputs": [],
   "source": [
    "# second_pull_df = pd.read_pickle('./second_set_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.710138Z",
     "start_time": "2023-02-04T23:48:29.695262Z"
    }
   },
   "outputs": [],
   "source": [
    "# third_pull_df.to_pickle('./third_set_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.725645Z",
     "start_time": "2023-02-04T23:48:29.710642Z"
    }
   },
   "outputs": [],
   "source": [
    "# third_pull_df = pd.read_pickle('./third_set_rows.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.740927Z",
     "start_time": "2023-02-04T23:48:29.726648Z"
    }
   },
   "outputs": [],
   "source": [
    "# full_df = pd.concat([first_pull_df, second_pull_df, third_pull_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.756932Z",
     "start_time": "2023-02-04T23:48:29.741930Z"
    }
   },
   "outputs": [],
   "source": [
    "# full_df.to_pickle('./full_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.772941Z",
     "start_time": "2023-02-04T23:48:29.757933Z"
    }
   },
   "outputs": [],
   "source": [
    "# new_full = pd.read_pickle('../full_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:37.916764Z",
     "start_time": "2023-02-04T23:48:29.773949Z"
    }
   },
   "outputs": [],
   "source": [
    "# sat_test = functions.get_important_info(sat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling the the Data in Batches and Saving to .pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0-499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:37.932278Z",
     "start_time": "2023-02-04T23:48:37.917774Z"
    }
   },
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_499 = sat_test[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/0_to_500.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500-999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_999 = sat_test[500:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/500_to_999.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000-1499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_1499 = sat_test[1000:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_1499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/1000_to_1499.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1500-1999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_1999 = sat_test[1500:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_1999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/1500_to_1999.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2000-2499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_2499 = sat_test[2000:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_2499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/2000_to_2499.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2500-2999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_2999 = sat_test[2500:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_2999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/2500_to_2999.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3000-3499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_3499 = sat_test[3000:3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_3499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/3000_to_3499.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3500-3999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_3999 = sat_test[3500:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_3999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/3500_to_3999.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4000-4499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_4499 = sat_test[4000:4500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_4499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/4000_to_4499.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4500-4999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_4999 = sat_test[4500:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_4999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/4500_to_4999.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5000-5499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_5499 = sat_test[5000:5500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_5499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/5000_to_5499.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5500-5999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_5999 = sat_test[5500:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_5999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/5500_to_5999.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6000-6510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df\n",
    "# small_sat_test_6510 = sat_test[6000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "# test_df = functions2.try_get_sat_to_features(small_sat_test_5999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "# test_df.to_pickle('../pickles/test_data/6000_to_6510.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Saved Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:16.449249Z",
     "start_time": "2023-02-05T17:32:16.000024Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_499 = pd.read_pickle('../pickles/test_data/0_to_500.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:25.334954Z",
     "start_time": "2023-02-05T17:32:24.976440Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_999 = pd.read_pickle('../pickles/test_data/500_to_999.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:28.932216Z",
     "start_time": "2023-02-05T17:32:28.480967Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_1499 = pd.read_pickle('../pickles/test_data/1000_to_1499.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:31.050968Z",
     "start_time": "2023-02-05T17:32:30.576597Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_1999 = pd.read_pickle('../pickles/test_data/1500_to_1999.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:33.917992Z",
     "start_time": "2023-02-05T17:32:32.896246Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_2499 = pd.read_pickle('../pickles/test_data/2000_to_2499.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:36.921193Z",
     "start_time": "2023-02-05T17:32:36.095912Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_2999 = pd.read_pickle('../pickles/test_data/2500_to_2999.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:38.720176Z",
     "start_time": "2023-02-05T17:32:37.783808Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_3499 = pd.read_pickle('../pickles/test_data/3000_to_3499.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:40.885113Z",
     "start_time": "2023-02-05T17:32:39.752074Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_3999 = pd.read_pickle('../pickles/test_data/3500_to_3999.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:42.081262Z",
     "start_time": "2023-02-05T17:32:41.840988Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_4499 = pd.read_pickle('../pickles/test_data/4000_to_4499.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:44.478106Z",
     "start_time": "2023-02-05T17:32:43.119824Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_4999 = pd.read_pickle('../pickles/test_data/4500_to_4999.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:46.892891Z",
     "start_time": "2023-02-05T17:32:45.344360Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_5499 = pd.read_pickle('../pickles/test_data/5000_to_5499.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:49.692868Z",
     "start_time": "2023-02-05T17:32:49.479665Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_5999 = pd.read_pickle('../pickles/test_data/5500_to_5999.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:54.148151Z",
     "start_time": "2023-02-05T17:32:52.279657Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_sat_test_6510 = pd.read_pickle('../pickles/test_data/6000_to_6510.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a list of all batched DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:55.517482Z",
     "start_time": "2023-02-05T17:32:55.512484Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_pickle_list = [small_sat_test_499, small_sat_test_999, small_sat_test_1499,\n",
    "#                    small_sat_test_1999, small_sat_test_2499, small_sat_test_2999,\n",
    "#                    small_sat_test_3499, small_sat_test_3999, small_sat_test_4499,\n",
    "#                    small_sat_test_4999, small_sat_test_5499, small_sat_test_5999,\n",
    "#                    small_sat_test_6510]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing the List of DataFrames together so that I can concat to one large DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:32:59.478234Z",
     "start_time": "2023-02-05T17:32:59.464841Z"
    }
   },
   "outputs": [],
   "source": [
    "# full_test_df = pd.concat(test_pickle_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:33:56.528364Z",
     "start_time": "2023-02-05T17:33:56.520857Z"
    }
   },
   "outputs": [],
   "source": [
    "# full_test_df = full_test_df.reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:34:31.271807Z",
     "start_time": "2023-02-05T17:34:27.528467Z"
    }
   },
   "outputs": [],
   "source": [
    "# full_test_df.to_pickle('../pickles/test_data/full_test_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.189139Z",
     "start_time": "2023-02-04T23:48:29.174292Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Can use this if I decide to use multiple satelitte images\n",
    "# def get_sat_info(df):\n",
    "\n",
    "#     '''\n",
    "#     input a dataframe and get a dictionary with satellite information for each row in the dataframe\n",
    "#     '''\n",
    "    \n",
    "#     sat_dict = {}\n",
    "#     for index in range(len(df)):\n",
    "#         row = df.iloc[index]\n",
    "\n",
    "#         # Get all satellite images\n",
    "#         search = catalog.search(collections=[\"sentinel-2-l2a\", \"landsat-c2-l2\"],\n",
    "#                                 bbox=row['bbox'],\n",
    "#                                 datetime=row['date_range'],\n",
    "#                                 query={'eo:cloud_cover': {'lt':100}}\n",
    "#     )\n",
    "\n",
    "\n",
    "#         # Going through Satellite info\n",
    "\n",
    "#         # search for sat images and create a dataframe with results for one sample\n",
    "# #         search_items = [item for item in search.get_all_items()]\n",
    "#         search_items = [item for item in search.item_collection()]\n",
    "\n",
    "\n",
    "#         pic_details = []\n",
    "#         for pic in search_items:\n",
    "#             pic_details.append(\n",
    "#             {\n",
    "#             'item': pic,\n",
    "#             'satelite_name':pic.collection_id,\n",
    "#             'img_date':pic.datetime.date(),\n",
    "#             'cloud_cover(%)': pic.properties['eo:cloud_cover'],\n",
    "#             'img_bbox': pic.bbox,\n",
    "#             'min_long': pic.bbox[0],\n",
    "#             \"max_long\": pic.bbox[2],\n",
    "#             \"min_lat\": pic.bbox[1],\n",
    "#             \"max_lat\": pic.bbox[3]\n",
    "#             }\n",
    "#             )\n",
    "\n",
    "#         temp_df = pd.DataFrame(pic_details)\n",
    "\n",
    "#         # Check to make sure sample location is actually within sat image\n",
    "#         temp_df['has_sample_point'] = (\n",
    "#             (temp_df.min_lat < row.latitude)\n",
    "#             & (temp_df.max_lat > row.latitude)\n",
    "#             & (temp_df.min_long < row.longitude)\n",
    "#             & (temp_df.max_long > row.longitude)\n",
    "#         )\n",
    "\n",
    "#         temp_df = temp_df[temp_df['has_sample_point'] == True]\n",
    "#         sat_dict[row['uid']] = temp_df\n",
    "        \n",
    "#     return sat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.205143Z",
     "start_time": "2023-02-04T23:48:29.190140Z"
    }
   },
   "outputs": [],
   "source": [
    "# # delete comments for prints (# is all on left edge)\n",
    "# def pick_best_sat(df, sat_dict):\n",
    "    \n",
    "#     '''\n",
    "#     input a dataframe and dictionary of satellite images and returns a dataframe with the best satellite image\n",
    "#     '''\n",
    "    \n",
    "#     # picking the best\n",
    "#     # inputs would need to be df and dictionary\n",
    "#     best_sat_df = pd.DataFrame()\n",
    "#     row_count=0\n",
    "#     invalid_sats = 0\n",
    "#     for index in range(len(df)):\n",
    "#         row = df.iloc[index]\n",
    "\n",
    "#         name = row['uid']\n",
    "#         temp_df = sat_dict[name]\n",
    "#         temp_df = temp_df.reset_index()\n",
    "#         # checking to see if there's only one image and adding it to df if so\n",
    "#         if len(temp_df) == 1:\n",
    "# #             print('only one satellite')\n",
    "#             temp_df = temp_df.reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#             row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#             row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#             best_sat_df = pd.concat([best_sat_df, row])\n",
    "#             row_count+=1\n",
    "\n",
    "#         # checking if no images\n",
    "#         elif len(temp_df) == 0:\n",
    "#             invalid_sats +=1\n",
    "#             row = pd.DataFrame(row).T.reset_index()\n",
    "#             row = row.set_index(pd.Series(row_count)).drop('index', axis=1)\n",
    "#             best_sat_df = pd.concat([best_sat_df, row])\n",
    "#             row_count+=1\n",
    "# #             print('no satellite images')\n",
    "#             continue\n",
    "\n",
    "#         # There are many satellite images, need to narrow it down\n",
    "#         else:\n",
    "# #             print('many sats')\n",
    "#             # first checking for any sentinel satelites\n",
    "#             if len(temp_df[temp_df['satelite_name'].str.contains('entinel')]) >0:\n",
    "#                     temp_df = temp_df[temp_df['satelite_name'].str.contains('entinel')]\n",
    "\n",
    "#                     # if only one sentinel, add to df and move on\n",
    "#                     if len(temp_df) == 1:\n",
    "# #                         print('\\tonly one sentinal')\n",
    "#                         temp_df = temp_df.reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#                         row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#                         row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#                         best_sat_df = pd.concat([best_sat_df, row])\n",
    "#                         row_count+=1\n",
    "#                     # if many sentinel, check for images with low cloud cover\n",
    "#                     else:\n",
    "# #                         print('\\tmany sentinel')\n",
    "#                         # checking for clouds less than 30%\n",
    "#                         if len(temp_df[temp_df['cloud_cover(%)'] <= 30]) >0:\n",
    "# #                             print('\\t\\tsentinal cloud cover lower than 30%')\n",
    "#                             temp_df = temp_df[temp_df['cloud_cover(%)'] <= 30]\n",
    "\n",
    "#                             # add the row with the closest date\n",
    "#                             temp_df = temp_df.sort_values('img_date', ascending=False).reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#                             temp_df = pd.DataFrame(temp_df.loc[0]).T\n",
    "#                             row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#                             row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#                             best_sat_df = pd.concat([best_sat_df, row])\n",
    "#                             row_count+=1\n",
    "#                         else:\n",
    "#                             # If there's only images with a clouds over 30%, \n",
    "#                             # pick the one with the least clouds\n",
    "# #                             print('\\t\\tvery cloudy sentinel')\n",
    "#                             temp_df = temp_df.sort_values('cloud_cover(%)', ascending=True).reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#                             temp_df = pd.DataFrame(temp_df.loc[0]).T\n",
    "#                             row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#                             row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#                             best_sat_df = pd.concat([best_sat_df, row])\n",
    "#                             row_count+=1\n",
    "\n",
    "#             else:\n",
    "# #                 print('\\tno sentinal')\n",
    "#                 if len(temp_df[temp_df['cloud_cover(%)'] <= 30]) >0:\n",
    "# #                     print('\\t\\tlandsat cloud cover lower than 30%')\n",
    "#                     temp_df = temp_df[temp_df['cloud_cover(%)'] <= 30]\n",
    "\n",
    "#                     # add the row with the closest date\n",
    "#                     temp_df = temp_df.sort_values('img_date', ascending=False).reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#                     temp_df = pd.DataFrame(temp_df.loc[0]).T\n",
    "#                     row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#                     row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#                     best_sat_df = pd.concat([best_sat_df, row])\n",
    "#                     row_count+=1\n",
    "#                 else:\n",
    "#                     # If there's only images with a clouds over 30%, \n",
    "#                     # pick the one with the least clouds\n",
    "# #                     print('\\t\\tvery cloudy landsat')\n",
    "#                     temp_df = temp_df.sort_values('cloud_cover(%)', ascending=True).reset_index().drop(['index','min_long', 'max_long', 'min_lat', 'max_lat'], axis=1)\n",
    "#                     temp_df = pd.DataFrame(temp_df.loc[0]).T\n",
    "#                     row = pd.DataFrame(row).T.reset_index().join(temp_df, how='outer')\n",
    "#                     row = row.set_index(pd.Series(row_count)).drop(['level_0', 'index'], axis=1)\n",
    "#                     best_sat_df = pd.concat([best_sat_df, row])\n",
    "#                     row_count+=1\n",
    "\n",
    "\n",
    "\n",
    "#     print(f'{len(df)} attempts. {invalid_sats} failures.')\n",
    "#     return best_sat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.220931Z",
     "start_time": "2023-02-04T23:48:29.206143Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_arrays_from_sats(df):\n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     input a dataframe with satellites in it and get a dictionary with arrays \n",
    "#     that came from cropped images around the sample area\n",
    "#     '''\n",
    "\n",
    "# # Now to get images from the satellites\n",
    "#     array_dict = {}\n",
    "#     scaler = functions.MinMaxScaler3D(feature_range=(0,255))\n",
    "#     error_count = 0\n",
    "#     attempt_count = 0\n",
    "#     for index in range(len(df)):\n",
    "#         row = df.iloc[index]\n",
    "\n",
    "\n",
    "#         try:\n",
    "#             attempt_count +=1\n",
    "#         # checking to see which satellite it came from\n",
    "#             if 'sentinel' in row['satelite_name']:\n",
    "#                 # Setting tiny crop box for image\n",
    "#                 minx, miny, maxx, maxy = row['tiny_crop_bbox']\n",
    "#                 # getting the image\n",
    "#                 image = rioxarray.open_rasterio(pc.sign(row['item'].assets[\"visual\"].href)).rio.clip_box(\n",
    "#                         minx=minx,\n",
    "#                         miny=miny,\n",
    "#                         maxx=maxx,\n",
    "#                         maxy=maxy,\n",
    "#                         crs=\"EPSG:4326\",\n",
    "#                     )\n",
    "\n",
    "#                 image_array = image.to_numpy()\n",
    "#                 img_array_trans = np.transpose(image_array, axes=[1, 2, 0])\n",
    "#                 # storing array of image in dictionary\n",
    "#                 array_dict[row['uid']] = img_array_trans\n",
    "\n",
    "#             else:\n",
    "#                 # getting the image from the LandSat satellite\n",
    "#                 minx, miny, maxx, maxy = row['tiny_crop_bbox']\n",
    "#                 image = odc.stac.stac_load(\n",
    "#                         [pc.sign(row['item'])], bands=[\"red\", \"green\", \"blue\"], bbox=[minx, miny, maxx, maxy]\n",
    "#                     ).isel(time=0)\n",
    "\n",
    "#                 image_array = image[[\"red\", \"green\", \"blue\"]].to_array()\n",
    "#                 img_array_trans = np.transpose(image_array.to_numpy(), axes=[1, 2, 0])\n",
    "#                 # scaling the image so its the same scale as the sentinel ones\n",
    "#                 scaled_img = scaler.fit_transform(img_array_trans)\n",
    "#         #         int_scaled_img = scaled_img.astype(int)\n",
    "#                 # storing array of image in dictionary\n",
    "#                 array_dict[row['uid']] = scaled_img\n",
    "                \n",
    "\n",
    "#         except:\n",
    "#             error_count +=1\n",
    "            \n",
    "            \n",
    "#     print(f'{attempt_count} attempted. {error_count} failures.')\n",
    "#     return array_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.236936Z",
     "start_time": "2023-02-04T23:48:29.221932Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_features(df, img_arrays):\n",
    "#     '''\n",
    "#     input a dataframe and a list of integers and create features from arrays\n",
    "#     '''\n",
    "#     feature_df = pd.DataFrame()\n",
    "#     for index in range(len(img_arrays.keys())):\n",
    "#         feature_dict = {}\n",
    "#         key =list(img_arrays.keys())[index]\n",
    "# #         row = df.iloc[index]\n",
    "#         temp_array = img_arrays[key]\n",
    "#         for n, color in enumerate(['red', 'green', 'blue']):\n",
    "#             feature_dict['uid'] = key\n",
    "#             feature_dict[f'{color}_mean'] = np.mean(temp_array[:,:,n])\n",
    "#             feature_dict[f'{color}_median'] = np.median(temp_array[:,:,n])\n",
    "#             feature_dict[f'{color}_max'] = np.max(temp_array[:,:,n])\n",
    "#             feature_dict[f'{color}_min'] = np.min(temp_array[:,:,n])\n",
    "#             feature_dict[f'{color}_sum'] = np.sum(temp_array[:,:,n])\n",
    "#             feature_dict[f'{color}_product'] = np.prod(temp_array[:,:,n])\n",
    "#         feature_df = pd.concat([feature_df, pd.DataFrame(feature_dict, index=[index])], )\n",
    "\n",
    "#     feature_df = df.merge(feature_df, how='outer', on='uid')\n",
    "#     return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.252937Z",
     "start_time": "2023-02-04T23:48:29.237937Z"
    }
   },
   "outputs": [],
   "source": [
    "# # A function to get it all in one\n",
    "# def get_sat_to_features(df):\n",
    "    \n",
    "#     '''\n",
    "#     input a dataframe of raw data and get sat images, convert to arrays, and turn into features.\n",
    "#     '''\n",
    "#     catalog = Client.open(\n",
    "#     \"https://planetarycomputer.microsoft.com/api/stac/v1\", modifier=pc.sign_inplace\n",
    "#     )\n",
    "    \n",
    "#     # get sat info\n",
    "#     satelite_dict = get_sat_info(df)\n",
    "    \n",
    "#     # pick best sat\n",
    "#     single_df = pick_best_sat(df, satelite_dict)\n",
    "    \n",
    "#     # get image arrays from best sats\n",
    "#     img_arrays = get_arrays_from_sats(single_df)\n",
    "    \n",
    "#     # get a dataframe with relevant features\n",
    "#     feature_df = get_features(single_df, img_arrays)\n",
    "    \n",
    "#     return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-04T23:48:29.268948Z",
     "start_time": "2023-02-04T23:48:29.253937Z"
    }
   },
   "outputs": [],
   "source": [
    "# def clean_data(df):\n",
    "#     '''\n",
    "#     input dataframe with all data and clean it.\n",
    "#     '''\n",
    "#     # only keeping cols that I need\n",
    "#     model_df = df[['date', 'latitude', 'longitude', 'season', 'img_date',\n",
    "#             'red_mean', 'red_median', 'red_max', 'red_min','red_sum',\n",
    "#             'red_product', 'green_mean', 'green_median', 'green_max',\n",
    "#             'green_min', 'green_sum', 'green_product', 'blue_mean',\n",
    "#             'blue_median','blue_max', 'blue_min', 'blue_sum', 'blue_product', 'severity']]\n",
    "#     # dropping nulls\n",
    "#     model_df = model_df.dropna()\n",
    "#     # converting to correct type\n",
    "#     model_df['date'] = model_df['date'].apply(lambda x: datetime.date(x))\n",
    "#     # getting difference from image date to sample date and creating feature\n",
    "#     model_df['days_from_sat_to_sample'] = model_df['date'] - model_df['img_date']\n",
    "#     # converting to int\n",
    "#     model_df['days_from_sat_to_sample'] = model_df['days_from_sat_to_sample'].dt.days\n",
    "#     # converting from datetime to an int\n",
    "#     model_df['date'] = model_df['date'].apply(lambda x: x.toordinal())\n",
    "#     model_df['img_date'] = model_df['img_date'].apply(lambda x: x.toordinal())\n",
    "#     # converting from string to float\n",
    "#     model_df['latitude'] = model_df['latitude'].apply(lambda x: x.astype(float))\n",
    "#     model_df['longitude'] = model_df['longitude'].apply(lambda x: x.astype(float))\n",
    "    \n",
    "#     # One hot encoding seasons\n",
    "#     ohe = OneHotEncoder(sparse=False)\n",
    "#     seasons = ohe.fit_transform(model_df[['season']])\n",
    "#     cols = ohe.get_feature_names_out()\n",
    "#     # converting new ohe to dataframe\n",
    "#     seasons = pd.DataFrame(seasons, columns=cols, index=model_df.index)\n",
    "#     model_ohe = pd.concat([model_df.drop('season', axis=1), seasons], axis=1)\n",
    "#     return model_ohe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (capstone)",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
